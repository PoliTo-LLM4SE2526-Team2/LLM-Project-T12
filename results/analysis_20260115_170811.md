# Results Analysis: results_20260115_170811.json

## Executive Summary

**Overall Performance:**
- **Official Score:** 0.72875 (slight decrease from previous 0.73)
- **Total Questions:** 400
- **Full Match:** 239 (59.75%)
- **Partial Match:** 105 (26.25%)
- **Incorrect:** 56 (14%)

## Critical Issue Discovered: Voting Mechanism Failure

### Problem Description
The voting details reveal a **major bug** in the LLM sampling process:
- Most questions show **4 out of 5 samples returning empty responses**
- Only sample #5 consistently returns content
- This results in option_votes showing mostly 0s or 1s instead of expected distribution across 5 samples

### Evidence Examples

**Question 12:**
```json
"option_votes": {"A": 0, "B": 0, "C": 0, "D": 0},
"voted_answers": [],
"final_answers": []
```
All 5 samples returned empty `selected_options: []` and `response: ""`

**Question 20:**
```json
"option_votes": {"A": 0, "B": 0, "C": 1, "D": 0},
"voted_answers": ["C"],
"final_answers": ["C"]
```
Only sample 5 returned content: `"selected_options": ["C"]`
Samples 1-4: empty

### Impact Analysis

**Voting Mechanism Breakdown:**
- **Expected:** 5 independent LLM calls, each selecting options → vote counts range 0-5
- **Actual:** Only 1 sample (#5) working → vote counts limited to 0-1
- **Consequence:** Vote threshold of 3 can NEVER be met with only 1 vote

**How the System Still Makes Predictions:**
Looking at the prediction text in error_cases:
```
Vote counts: A:0, B:0, C:0, D:0
Voted answers: []
After post-processing: ['A']
```

The system falls back to a post-processing step that still produces predictions even when voting fails. This explains:
1. Why the model still got 59.75% full match despite broken voting
2. Why performance dropped slightly (0.73 → 0.72875) - degraded from 5-sample consensus to single-sample decision

## Performance Metrics

### Option-Level Statistics

| Option | Precision | Recall | F1 Score |
|--------|-----------|--------|----------|
| A      | 86.1%     | 75.2%  | 0.8027   |
| B      | 86.7%     | 67.6%  | 0.7589   |
| C      | 90.4%     | 70.5%  | 0.7917   |
| D      | 88.4%     | 69.1%  | 0.7759   |

**Key Observations:**
- **Precision is high** (86-90%) - when the model selects an option, it's usually correct
- **Recall is moderate** (68-75%) - the model misses about 25-32% of correct options
- **Precision-Recall Gap:** 11-23 percentage points suggests conservative predictions

### Macro F1 Score: 0.7825

This is the average F1 across all options, showing balanced performance across the option set.

## Prediction Type Distribution

From the summary section:
```json
"prediction_types": {
  "full_match": 239,
  "partial_match": 105,
  "wrong_only": 38,
  "over_complete": 11,
  "mixed_error": 7
}
```

**Analysis:**
- **59.75% Full Match** - excellent when the model gets it right
- **26.25% Partial Match** - model identifies some but not all correct options
  - This is a HUGE category suggesting the model is too conservative
  - With proper voting, this could be improved
- **9.5% Wrong Only** - completely incorrect selections
- **2.75% Over-complete** - includes correct + incorrect options
- **1.75% Mixed Error** - both false positives and false negatives

## Root Cause Analysis

### Most Likely Causes of Empty Responses

1. **API Timeout/Rate Limiting:**
   - Samples 1-4 might be hitting API limits
   - Sample 5 succeeds because it's the last attempt

2. **Concurrency Issues:**
   - If samples run in parallel, 4 might fail due to race conditions
   - Need to check the concurrent execution code in [src/approaches.py](src/approaches.py)

3. **Temperature/Generation Issues:**
   - With temperature=0.5, some samples might generate empty/malformed output
   - LLM might be failing to follow the expected format 80% of the time

4. **Prompt/Parsing Issues:**
   - The prompt might not be clear enough
   - Response parsing might be failing silently for samples 1-4

## Actionable Recommendations

### Immediate Actions

1. **Fix the Voting Mechanism:**
   ```python
   # Check in src/approaches.py - SelfConsistencyRefinementApproach.solve()
   # Add error handling and logging for each sample iteration
   # Verify all 5 samples are actually getting responses
   ```

2. **Add Debug Logging:**
   - Log each sample's raw LLM response
   - Log parsing failures
   - Track which samples succeed vs fail

3. **Verify LLM Call:**
   - Check if concurrent calls are being made properly
   - Verify API rate limits aren't being hit
   - Add retry logic for failed samples

### Investigation Steps

1. **Check the approach code:**
   - Review how samples are generated in the loop
   - Verify response parsing logic
   - Check error handling

2. **Review LLM module:**
   - Inspect [src/llm.py](src/llm.py) for API call logic
   - Check timeout settings
   - Verify retry mechanisms

3. **Test with debug mode:**
   - Run on a small subset (5-10 questions)
   - Enable verbose logging
   - Manually inspect all 5 responses per question

## Expected vs Actual Behavior

### Expected (Proper Voting):
```
Question X:
  Sample 1: A, B → votes: A+1, B+1
  Sample 2: A, C → votes: A+2, B+1, C+1
  Sample 3: A, B → votes: A+3, B+2, C+1
  Sample 4: A, D → votes: A+4, B+2, C+1, D+1
  Sample 5: A, B → votes: A+5, B+3, C+1, D+1
  
Final: A (5 votes ≥ threshold 3), B (3 votes ≥ threshold 3)
```

### Actual (Broken):
```
Question X:
  Sample 1: [] → no votes
  Sample 2: [] → no votes
  Sample 3: [] → no votes
  Sample 4: [] → no votes
  Sample 5: A, B → votes: A+1, B+1
  
Final: Falls back to post-processing (selects A based on "best response")
```

## Performance Comparison

If the voting was working properly with all 5 samples:
- Expected improvement: 3-5 percentage points (based on partial_match reduction)
- Better coverage of multi-option answers
- More stable predictions through majority voting

Current performance (0.72875) is **artificially limited** by the broken voting mechanism.

## Next Steps

1. **Diagnose the empty response issue** - highest priority
2. **Fix the concurrent execution** in src/approaches.py
3. **Add comprehensive logging** to track sample generation
4. **Re-run experiments** with fixed code
5. **Compare results** to see actual impact of proper voting

## Files to Investigate

1. [src/approaches.py](src/approaches.py) - Line ~150-250 (SelfConsistencyRefinementApproach.solve)
2. [src/llm.py](src/llm.py) - API call and retry logic
3. [src/prompts.py](src/prompts.py) - Verify prompt format is consistent

---

**Conclusion:** The voting mechanism is fundamentally broken, causing 80% of samples to return empty responses. Despite this, the model achieves 72.88% score through fallback mechanisms, suggesting that **fixing the voting will significantly improve performance** (estimated 75-78% score if all samples work properly).
