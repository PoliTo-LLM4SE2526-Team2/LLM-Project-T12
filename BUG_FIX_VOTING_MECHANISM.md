# ğŸ”´ æŠ•ç¥¨æœºåˆ¶å¤±æ•ˆé—®é¢˜è¯Šæ–­ä¸ä¿®å¤æ–¹æ¡ˆ

## é—®é¢˜ç¡®è®¤

æ ¹æ® `results_20260115_170811.json` åˆ†æï¼Œå‘ç°ï¼š

### ç—‡çŠ¶
- **80%çš„æ ·æœ¬è¿”å›ç©ºå“åº”** (æ ·æœ¬1-4å‡ ä¹éƒ½æ˜¯ç©º)
- **åªæœ‰æ ·æœ¬5å¶å°”æœ‰å†…å®¹**
- **æŠ•ç¥¨è®¡æ•°æ°¸è¿œæ˜¯0-1ï¼Œæ— æ³•è¾¾åˆ°é˜ˆå€¼3**

### å®é™…å½±å“
```json
// å…¸å‹æ¡ˆä¾‹
"option_votes": {"A": 0, "B": 0, "C": 1, "D": 0},
"voted_answers": ["C"],
"voting_details": [
  {"sample_id": 1, "selected_options": [], "response": ""},  // âŒ ç©º
  {"sample_id": 2, "selected_options": [], "response": ""},  // âŒ ç©º
  {"sample_id": 3, "selected_options": [], "response": ""},  // âŒ ç©º
  {"sample_id": 4, "selected_options": [], "response": ""},  // âŒ ç©º
  {"sample_id": 5, "selected_options": ["C"], "response": "..."}  // âœ… å”¯ä¸€æœ‰æ•ˆ
]
```

## æ ¹æœ¬åŸå› 

### 1. é”™è¯¯å¤„ç†è¿‡äºç®€åŒ– (src/llm.py)

```python
# âŒ é—®é¢˜ä»£ç 
def generate(self, messages, temperature=0, top_p=1) -> str:
    try:
        response = self.client.chat.completions.create(...)
        return response.choices[0].message.content
    except Exception as e:
        print(f"API Error: {e}")  # åªæ‰“å°é”™è¯¯
        return ""  # ç›´æ¥è¿”å›ç©ºå­—ç¬¦ä¸²ï¼Œæ²¡æœ‰é‡è¯•ï¼
```

**åæœï¼š**
- APIè°ƒç”¨å¤±è´¥ â†’ è¿”å›ç©ºå­—ç¬¦ä¸²
- æŠ•ç¥¨æœºåˆ¶æ”¶åˆ°ç©ºå“åº” â†’ è§£æä¸ºç©ºåˆ—è¡¨
- æ— æŠ¥è­¦ã€æ— é‡è¯•ã€æ— æ—¥å¿—

### 2. å¯èƒ½çš„å¤±è´¥åŸå› 

#### A. APIé€Ÿç‡é™åˆ¶
```
å¹¶å‘5ä¸ªè¯·æ±‚åŒæ—¶å‘å‡º â†’ å‰4ä¸ªè¢«é™æµ â†’ åªæœ‰ç¬¬5ä¸ªæˆåŠŸ
```

#### B. è¶…æ—¶é—®é¢˜
```
LLMå“åº”æ—¶é—´è¿‡é•¿ â†’ å‰å‡ ä¸ªè¯·æ±‚è¶…æ—¶ â†’ æ²¡æœ‰è®¾ç½®timeoutå‚æ•°
```

#### C. ä»¤ç‰Œé…é¢è€—å°½
```
token quota exceeded â†’ APIæ‹’ç» â†’ è¿”å›é”™è¯¯ä½†è¢«åæ‰
```

#### D. ç½‘ç»œä¸ç¨³å®š
```
é—´æ­‡æ€§ç½‘ç»œé—®é¢˜ â†’ éƒ¨åˆ†è¯·æ±‚å¤±è´¥ â†’ æ— é‡è¯•æœºåˆ¶
```

## ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šæ·»åŠ é‡è¯•æœºåˆ¶ï¼ˆæ¨èï¼‰

```python
# src/llm.py ä¿®æ”¹
from tenacity import retry, stop_after_attempt, wait_exponential
import logging

class ChatLLM(BaseLLM):
    def __init__(self, model_name: str, api_key: str, base_url: str):
        self.model_name = model_name
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.logger = logging.getLogger(__name__)

    @retry(
        stop=stop_after_attempt(3),  # æœ€å¤šé‡è¯•3æ¬¡
        wait=wait_exponential(multiplier=1, min=2, max=10),  # æŒ‡æ•°é€€é¿
        reraise=True
    )
    def generate(self, messages, temperature=0, top_p=1, timeout=60) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                timeout=timeout  # æ·»åŠ è¶…æ—¶æ§åˆ¶
            )
            content = response.choices[0].message.content
            
            # éªŒè¯å“åº”ä¸ä¸ºç©º
            if not content or not content.strip():
                self.logger.warning("Received empty response from API")
                raise ValueError("Empty response from API")
            
            return content
            
        except Exception as e:
            self.logger.error(f"API Error (attempt failed): {e}")
            raise  # æŠ›å‡ºå¼‚å¸¸è®©retryå¤„ç†
```

**å®‰è£…ä¾èµ–ï¼š**
```bash
pip install tenacity
```

### æ–¹æ¡ˆ2ï¼šé¡ºåºè°ƒç”¨è€Œéå¹¶å‘

å¦‚æœæ˜¯APIé™æµå¯¼è‡´çš„ï¼Œå¯ä»¥æ”¹ä¸ºé¡ºåºè°ƒç”¨ï¼š

```python
# src/approaches.py - SelfConsistencyRefinementApproach.solve()
for i in range(self.num_samples):
    messages = [...]
    
    response = self.llm.generate(messages, temperature=self.temperature)
    
    # æ·»åŠ éªŒè¯å’Œæ—¥å¿—
    if not response or not response.strip():
        print(f"âš ï¸  WARNING: Sample {i+1} returned empty response!")
        # å¯ä»¥é€‰æ‹©ï¼š
        # 1. é‡è¯•
        # 2. è·³è¿‡
        # 3. ä½¿ç”¨é»˜è®¤å€¼
        continue  # æˆ– retry logic
    
    all_responses.append(response)
    # ... æŠ•ç¥¨é€»è¾‘
    
    # æ·»åŠ å»¶è¿Ÿé¿å…é™æµ
    if i < self.num_samples - 1:  # æœ€åä¸€ä¸ªä¸éœ€è¦å»¶è¿Ÿ
        time.sleep(0.5)  # 500mså»¶è¿Ÿ
```

### æ–¹æ¡ˆ3ï¼šå¹¶å‘æ§åˆ¶+é”™è¯¯å¤„ç†

ä½¿ç”¨çº¿ç¨‹æ± ä½†æ§åˆ¶å¹¶å‘æ•°ï¼š

```python
import concurrent.futures
import time

def _generate_sample(self, i, system_prompt, user_prompt):
    """ç”Ÿæˆå•ä¸ªæ ·æœ¬ï¼ˆæ”¯æŒå¹¶å‘ï¼‰"""
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    try:
        response = self.llm.generate(messages, temperature=self.temperature)
        if not response:
            return i, None, "Empty response"
        
        answers = self._parse_answer_from_response(response)
        return i, response, answers
    except Exception as e:
        return i, None, str(e)

# åœ¨ solve() ä¸­ä½¿ç”¨
with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:  # é™åˆ¶å¹¶å‘æ•°
    futures = {
        executor.submit(self._generate_sample, i, system_prompt, user_prompt): i 
        for i in range(self.num_samples)
    }
    
    for future in concurrent.futures.as_completed(futures):
        i, response, result = future.result()
        
        if response is None:
            print(f"âš ï¸  Sample {i+1} failed: {result}")
            # é‡è¯•é€»è¾‘æˆ–è·³è¿‡
            continue
        
        all_responses.append(response)
        # ... æŠ•ç¥¨é€»è¾‘
```

## ç«‹å³éªŒè¯æ­¥éª¤

### Step 1: æ£€æŸ¥æ˜¯å¦æœ‰APIé”™è¯¯æ—¥å¿—

è¿è¡Œæ—¶æŸ¥çœ‹æ§åˆ¶å°è¾“å‡ºï¼Œçœ‹æ˜¯å¦æœ‰ "API Error:" ä¿¡æ¯ï¼š

```bash
# è¿è¡Œå°‘é‡æµ‹è¯•
python run.py --data_path data/dev --approach sc_refine --prompt conservative --output results/debug_test.json --max_questions 5
```

è§‚å¯Ÿè¾“å‡ºä¸­æ˜¯å¦æœ‰ï¼š
- `API Error: ...` 
- `Sample 1: No answer`
- `Sample 2: No answer`
- ç­‰ç­‰

### Step 2: æ·»åŠ ä¸´æ—¶è°ƒè¯•æ—¥å¿—

åœ¨ `src/llm.py` ä¸­ä¸´æ—¶æ·»åŠ ï¼š

```python
def generate(self, messages, temperature=0, top_p=1) -> str:
    print(f"ğŸ”µ Calling API with temp={temperature}...")  # è°ƒè¯•æ—¥å¿—
    try:
        response = self.client.chat.completions.create(...)
        content = response.choices[0].message.content
        print(f"âœ… API returned {len(content)} chars")  # è°ƒè¯•æ—¥å¿—
        return content
    except Exception as e:
        print(f"âŒ API Error: {e}")  # æ”¹è¿›é”™è¯¯ä¿¡æ¯
        print(f"   Messages: {messages[0]['role']}, length={len(messages[0]['content'])}")
        return ""
```

### Step 3: éªŒè¯APIé…é¢å’Œé™æµ

æ£€æŸ¥ `.env` æ–‡ä»¶ä¸­çš„APIé…ç½®ï¼š

```bash
# ä½¿ç”¨curlæµ‹è¯•APIæ˜¯å¦æ­£å¸¸
curl -X POST https://your-api-endpoint/v1/chat/completions \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "your-model",
    "messages": [{"role": "user", "content": "test"}]
  }'
```

## æ¨èä¿®å¤ä¼˜å…ˆçº§

### ğŸ”´ ä¼˜å…ˆçº§1ï¼šæ·»åŠ é”™è¯¯æ—¥å¿—å’ŒéªŒè¯ï¼ˆç«‹å³ï¼‰
```python
# src/llm.py
except Exception as e:
    import traceback
    print(f"âŒ API Error: {e}")
    print(f"   Traceback: {traceback.format_exc()}")
    return ""
```

### ğŸŸ  ä¼˜å…ˆçº§2ï¼šæ·»åŠ é‡è¯•æœºåˆ¶ï¼ˆ1å°æ—¶å†…ï¼‰
ä½¿ç”¨ `tenacity` åº“å®ç°è‡ªåŠ¨é‡è¯•

### ğŸŸ¡ ä¼˜å…ˆçº§3ï¼šä¼˜åŒ–å¹¶å‘ç­–ç•¥ï¼ˆä»Šå¤©å†…ï¼‰
- é™åˆ¶å¹¶å‘æ•°ä¸º2
- æ·»åŠ å»¶è¿Ÿé¿å…é™æµ

### ğŸŸ¢ ä¼˜å…ˆçº§4ï¼šå®Œå–„ç›‘æ§ï¼ˆåç»­ï¼‰
- è®°å½•æ¯ä¸ªæ ·æœ¬çš„æˆåŠŸ/å¤±è´¥ç‡
- ç»Ÿè®¡APIè°ƒç”¨å»¶è¿Ÿ
- ç›‘æ§tokenä½¿ç”¨é‡

## é¢„æœŸæ”¹è¿›æ•ˆæœ

ä¿®å¤åï¼š
- **æ ·æœ¬æˆåŠŸç‡ï¼š** 20% â†’ 100%
- **æŠ•ç¥¨æœ‰æ•ˆæ€§ï¼š** æ‰€æœ‰5ä¸ªæ ·æœ¬éƒ½å‚ä¸æŠ•ç¥¨
- **æ€§èƒ½æå‡ï¼š** é¢„è®¡ +3~5 ä¸ªç™¾åˆ†ç‚¹ (0.728 â†’ 0.75-0.78)
- **Partial Matchå‡å°‘ï¼š** æ›´å¤šæ­£ç¡®é€‰é¡¹è¢«å¤šæ•°æŠ•ç¥¨é€‰ä¸­

## ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. âœ… **å·²å®Œæˆï¼š** é—®é¢˜è¯Šæ–­
2. â³ **è¿›è¡Œä¸­ï¼š** ç­‰å¾…ç”¨æˆ·ç¡®è®¤ä¿®å¤æ–¹å‘
3. ğŸ”œ **å¾…æ‰§è¡Œï¼š** 
   - [ ] æ·»åŠ è°ƒè¯•æ—¥å¿—è¿è¡Œæµ‹è¯•
   - [ ] å®ç°é‡è¯•æœºåˆ¶
   - [ ] é‡æ–°è¿è¡Œå®éªŒ
   - [ ] å¯¹æ¯”ä¿®å¤å‰åæ€§èƒ½

---

**ç»“è®ºï¼š** æŠ•ç¥¨æœºåˆ¶ä»£ç æœ¬èº«æ˜¯æ­£ç¡®çš„ï¼Œé—®é¢˜å‡ºåœ¨APIè°ƒç”¨å±‚çš„é”™è¯¯å¤„ç†è¿‡äºç®€å•ï¼Œå¯¼è‡´å¤±è´¥çš„è¯·æ±‚è¢«é™é»˜å¿½ç•¥ã€‚ä¿®å¤åï¼ŒSelf-ConsistencyæŠ•ç¥¨å°†çœŸæ­£å‘æŒ¥ä½œç”¨ã€‚
